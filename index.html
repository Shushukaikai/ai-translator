<!DOCTYPE html>
<html lang="zh-CN">
<head>
  <meta charset="UTF-8" />
  <meta name="viewport" content="width=device-width, initial-scale=1.0" />
  <title>AIè¯­éŸ³ç¿»è¯‘åŠ©æ‰‹ï¼ˆWhisperä¼˜åŒ–ç‰ˆï¼‰</title>
  <script src="https://cdn.tailwindcss.com"></script>
</head>
<body class="bg-gray-100 p-6">
  <h1 class="text-2xl font-bold mb-4">ğŸ¤ AIè¯­éŸ³ç¿»è¯‘åŠ©æ‰‹ï¼ˆWhisperä¼˜åŒ–ç‰ˆï¼‰</h1>
  <button id="recordBtn" class="bg-blue-500 text-white px-4 py-2 rounded mb-4">ğŸ™ å¼€å§‹å½•éŸ³</button>
  <div id="status" class="text-sm text-gray-500 mb-2">ç­‰å¾…å½•éŸ³ä¸­...</div>

  <div class="mb-2 text-base font-semibold text-gray-800">
    <span class="font-bold text-gray-600">ä½ è¯´çš„æ˜¯ï¼š</span><span id="spokenText">ï¼ˆå°šæœªè¯†åˆ«ï¼‰</span>
  </div>
  <div class="mb-4 text-lg font-bold text-blue-700">
    <span class="font-bold text-gray-600">ç¿»è¯‘ç»“æœï¼š</span><span id="translatedText">ï¼ˆå°šæœªç¿»è¯‘ï¼‰</span>
  </div>

  <script>
    let mediaRecorder;
    let audioChunks = [];

    const recordBtn = document.getElementById('recordBtn');
    const status = document.getElementById('status');
    const spokenText = document.getElementById('spokenText');
    const translatedText = document.getElementById('translatedText');

    recordBtn.onclick = async () => {
      if (!navigator.mediaDevices) {
        alert("ä½ çš„æµè§ˆå™¨ä¸æ”¯æŒå½•éŸ³");
        return;
      }

      const stream = await navigator.mediaDevices.getUserMedia({ audio: true });
      mediaRecorder = new MediaRecorder(stream);
      audioChunks = [];

      mediaRecorder.ondataavailable = e => audioChunks.push(e.data);

      mediaRecorder.onstop = async () => {
        const blob = new Blob(audioChunks, { type: 'audio/webm' });
        status.innerText = 'ä¸Šä¼ ä¸­...';

        spokenText.innerText = 'è¯†åˆ«ä¸­...';
        translatedText.innerText = 'ç¿»è¯‘å¤„ç†ä¸­...';

        const formData = new FormData();
        formData.append('file', blob, 'audio.webm');

        try {
          const res = await fetch('https://whisper-deepseek-demo.vercel.app/api/whisper', {
            method: 'POST',
            body: formData
          });

          const data = await res.json();
          spokenText.innerText = data.text || 'ï¼ˆæœªè¯†åˆ«åˆ°è¯­éŸ³ï¼‰';
          translatedText.innerText = data.translation || 'ï¼ˆç¿»è¯‘å¤±è´¥ï¼‰';

          const utter = new SpeechSynthesisUtterance(data.translation);
          utter.lang = /[\u4e00-\u9fa5]/.test(data.translation) ? 'zh-CN' : 'en-US';
          speechSynthesis.speak(utter);
        } catch (err) {
          translatedText.innerText = 'âš ï¸ ç¿»è¯‘å¤±è´¥ï¼Œè¯·ç¨åé‡è¯•';
        }

        // é‡ç½®æŒ‰é’®è¡Œä¸º
        recordBtn.innerText = 'ğŸ™ é‡æ–°å½•éŸ³';
        recordBtn.onclick = () => location.reload();
        stream.getTracks().forEach(track => track.stop());
      };

      mediaRecorder.start();
      status.innerText = 'ğŸ™ å½•éŸ³ä¸­... è¯·è¯´è¯';
      recordBtn.innerText = 'ğŸ›‘ åœæ­¢å½•éŸ³';

      // åŠ¨æ€ç»‘å®šåœæ­¢é€»è¾‘
      recordBtn.onclick = () => {
        mediaRecorder.stop();
        status.innerText = 'â³ å¤„ç†ä¸­...';
        recordBtn.disabled = true;
      };
    };
  </script>
</body>
</html>
