<!DOCTYPE html>
<html lang="zh-CN">
<head>
  <meta charset="UTF-8" />
  <meta name="viewport" content="width=device-width, initial-scale=1.0" />
  <title>AIè¯­éŸ³ç¿»è¯‘åŠ©æ‰‹ï¼ˆWhisperç‰ˆï¼‰</title>
  <script src="https://cdn.tailwindcss.com"></script>
</head>
<body class="bg-gray-100 p-6">
  <h1 class="text-2xl font-bold mb-4">ğŸ¤ AIè¯­éŸ³ç¿»è¯‘åŠ©æ‰‹ï¼ˆWhisperç‰ˆï¼‰</h1>
  <button id="recordBtn" class="bg-blue-500 text-white px-4 py-2 rounded mb-4">ğŸ™ å¼€å§‹å½•éŸ³</button>
  <div id="status" class="text-sm text-gray-500 mb-2">ç­‰å¾…å½•éŸ³ä¸­...</div>
  <div id="result" class="text-base font-semibold mb-2"></div>
  <div id="translated" class="text-green-600 font-bold text-lg mb-4"></div>

  <script>
    let mediaRecorder;
    let audioChunks = [];

    const recordBtn = document.getElementById('recordBtn');
    const status = document.getElementById('status');
    const result = document.getElementById('result');
    const translated = document.getElementById('translated');

    recordBtn.onclick = async () => {
      if (!navigator.mediaDevices) {
        alert("ä½ çš„æµè§ˆå™¨ä¸æ”¯æŒå½•éŸ³");
        return;
      }

      const stream = await navigator.mediaDevices.getUserMedia({ audio: true });
      mediaRecorder = new MediaRecorder(stream);
      audioChunks = [];

      mediaRecorder.ondataavailable = e => audioChunks.push(e.data);

      mediaRecorder.onstop = async () => {
        const blob = new Blob(audioChunks, { type: 'audio/webm' });
        status.innerText = 'ä¸Šä¼ ä¸­...';

        const formData = new FormData();
        formData.append('file', blob, 'audio.webm');

        try {
          const res = await fetch('https://whisper-deepseek-demo.vercel.app/api/whisper', {
            method: 'POST',
            body: formData
          });

          const data = await res.json();
          result.innerText = 'ä½ è¯´çš„æ˜¯ï¼š' + data.text;
          translated.innerText = 'ç¿»è¯‘ï¼š' + data.translation;

          const utter = new SpeechSynthesisUtterance(data.translation);
          utter.lang = /[\u4e00-\u9fa5]/.test(data.translation) ? 'zh-CN' : 'en-US';
          speechSynthesis.speak(utter);
        } catch (err) {
          translated.innerText = 'âš ï¸ è¯·æ±‚å¤±è´¥ï¼Œè¯·é‡è¯•';
        }

        // é‡ç½®æŒ‰é’®è¡Œä¸º
        recordBtn.innerText = 'ğŸ™ é‡æ–°å½•éŸ³';
        recordBtn.onclick = () => location.reload();
        stream.getTracks().forEach(track => track.stop());
      };

      mediaRecorder.start();
      status.innerText = 'ğŸ™ å½•éŸ³ä¸­... è¯·è¯´è¯';
      recordBtn.innerText = 'ğŸ›‘ åœæ­¢å½•éŸ³';

      // åŠ¨æ€ç»‘å®šåœæ­¢é€»è¾‘
      recordBtn.onclick = () => {
        mediaRecorder.stop();
        status.innerText = 'â³ å¤„ç†ä¸­...';
        recordBtn.disabled = true;
      };
    };
  </script>
</body>
</html>
